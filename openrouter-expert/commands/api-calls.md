# ğŸ”§ OpenRouter API í˜¸ì¶œ ëª…ë ¹ì–´ ëª¨ìŒ

> ë¬´ë£Œ ëª¨ë¸ ì¤‘ì‹¬ì˜ API í˜¸ì¶œ ì˜ˆì œ

---

## ğŸ†“ ë¬´ë£Œ ëª¨ë¸ ê¸°ë³¸ í˜¸ì¶œ

### Python - requests ì‚¬ìš©
```python
import os
import requests

response = requests.post(
    "https://openrouter.ai/api/v1/chat/completions",
    headers={
        "Authorization": f"Bearer {os.getenv('OPENROUTER_API_KEY')}",
        "Content-Type": "application/json"
    },
    json={
        "model": "meta-llama/llama-3.1-8b-instruct:free",
        "messages": [
            {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}
        ]
    }
)

print(response.json()['choices'][0]['message']['content'])
```

### Python - OpenAI SDK ì‚¬ìš© (ê¶Œì¥)
```python
import os
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

response = client.chat.completions.create(
    model="meta-llama/llama-3.1-8b-instruct:free",
    messages=[
        {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}
    ]
)

print(response.choices[0].message.content)
```

---

## ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨ í˜¸ì¶œ

```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-8b-instruct:free",
    messages=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¨ê¹Œìš”?"}
    ]
)
```

---

## ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

### ì‹¤ì‹œê°„ ì¶œë ¥
```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-8b-instruct:free",
    messages=[
        {"role": "user", "content": "ê¸´ ì´ì•¼ê¸°ë¥¼ í•´ì£¼ì„¸ìš”."}
    ],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

## ğŸ›¡ï¸ ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨ í˜¸ì¶œ

### Rate Limit ëŒ€ì‘
```python
import time

def call_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="meta-llama/llama-3.1-8b-instruct:free",
                messages=messages
            )
            return response.choices[0].message.content
        except Exception as e:
            if "429" in str(e):  # Rate Limit ì—ëŸ¬
                wait_time = 2 ** attempt  # 1ì´ˆ, 2ì´ˆ, 4ì´ˆ
                print(f"Rate Limit! {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(wait_time)
            else:
                raise e
    return None

# ì‚¬ìš©
result = call_with_retry([{"role": "user", "content": "ì•ˆë…•!"}])
print(result)
```

---

## ğŸ“Š ì‚¬ìš©ëŸ‰ í™•ì¸

### ì‘ë‹µì—ì„œ í† í° ì‚¬ìš©ëŸ‰ í™•ì¸
```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-8b-instruct:free",
    messages=[{"role": "user", "content": "ê°„ë‹¨í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."}]
)

# í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥
usage = response.usage
print(f"ì…ë ¥ í† í°: {usage.prompt_tokens}")
print(f"ì¶œë ¥ í† í°: {usage.completion_tokens}")
print(f"ì´ í† í°: {usage.total_tokens}")
```

---

## ğŸ¯ ë¬´ë£Œ ëª¨ë¸ ëª©ë¡ í™•ì¸

```python
# ë¬´ë£Œ ëª¨ë¸ ì˜ˆì‹œ (ëª¨ë¸ëª… ë’¤ì— :free)
FREE_MODELS = [
    "meta-llama/llama-3.1-8b-instruct:free",
    "google/gemma-2-9b-it:free",
    # ìµœì‹  ëª©ë¡ì€ openrouter.ai/models ì—ì„œ í™•ì¸
]
```

---

## âš™ï¸ íŒŒë¼ë¯¸í„° ì¡°ì ˆ

### Temperature (ì°½ì˜ì„±)
```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-8b-instruct:free",
    messages=[{"role": "user", "content": "ì°½ì˜ì ì¸ ì´ì•¼ê¸° í•´ì£¼ì„¸ìš”."}],
    temperature=0.8  # 0.0 (ê²°ì •ì ) ~ 1.0 (ì°½ì˜ì )
)
```

### Max Tokens (ìµœëŒ€ ì¶œë ¥ ê¸¸ì´)
```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-8b-instruct:free",
    messages=[{"role": "user", "content": "ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”."}],
    max_tokens=100  # ì¶œë ¥ í† í° ì œí•œ
)
```

---

## ğŸ’¡ íŒ

```
âœ… ë¬´ë£Œ ëª¨ë¸ì€ ë°˜ë“œì‹œ :free ì ‘ë¯¸ì‚¬ í™•ì¸
âœ… í™˜ê²½ ë³€ìˆ˜ë¡œ API í‚¤ ê´€ë¦¬
âœ… ì—ëŸ¬ ì²˜ë¦¬ë¡œ Rate Limit ëŒ€ì‘
âœ… ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ
```
